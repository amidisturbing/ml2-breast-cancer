---
title: "logistic regression v1"
author:
- Silke Meiner
- Rafaela Neff
output:
  html_document:
    toc: yes
    df_print: paged
  html_notebook: default
  pdf_document:
    toc: yes
    fig_caption: yes
---
Read data without ID and empty X variables.
```{r}
#change working directory to current dir
wd = getwd()
setwd(wd)
#read data
ds <- read.csv('data/data.csv')[c(-33,-1)] # exclude ID and X
dim(ds)
str(ds)
```
# train / test split
We should have a separate folder (or just a separate data frame) for the test data. This should be the same for both methods to allow for a correct comparison.
# mathematical explanation of method
# preprocessing
Normalization. Scaling.
Should we check somewhere if our data is normal (normal distribution)? Does this matter? If so, put this in the math section
Make diagnosis a 0/1 variable:
```{r}
ds$diagnosis <- as.factor(ds$diagnosis)
levels(ds$diagnosis) <- c('0','1')
```
# Baseline Model
Run a logistic regression.
```{r}
glm.1 <- glm(diagnosis~. , data=ds, family='binomial')
summary(glm.1)
```

Error message: algorithm did not converge! That's not good.
cf https://stackoverflow.com/questions/61418709/warning-in-glm-analysis
The underlying problem is called complete separation or perfect prediction: The underlying problem is too simple for an advanced algorithm like logistic regression.

```{r}
ds$diagnosis <- as.integer(ds$diagnosis) -1
#levels(ds$diagnosis) <- c('0','1')
glm.2 <- lm(diagnosis~. , data=ds)
summary(glm.2)
```
The linear model works. But the result is not so good (RSE>0.2, R squared<0.8). Not exactly perfect prediction like. But the p-value in the end is real small: 2.2e-16?? wops, basically 0.
We would need a test set to get a decision on how good the model is?!

# Feature selection
# Metrics and HPO
Metrics: Accuracy, F1, precision and recall separately
Only hyperparameter: The threshold?!
