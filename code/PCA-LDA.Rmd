---
title: "Untitled"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# preparation

like reading the data and doing the basic transforms. Creating train and test (val, actually) sets
```{r}
#change working directory to current dir
wd = getwd()
setwd(wd)
#read data
ds <- read.csv('data/data.csv')[c(-33,-1)] # exclude ID and X

ds$diagnosis <- as.factor(ds$diagnosis)
levels(ds$diagnosis) <- list("0"="B", "1"="M")

library(caTools)
# seed should result in the same splitting as in the other R-norebook
set.seed(0)
train_rows = sample.split(ds$diagnosis, SplitRatio=0.7)
train = ds[ train_rows,]
test  = ds[!train_rows,]
```

# PCA
```{r}
pca <- prcomp(train[-1], center=TRUE, scale=TRUE)
```

The following is optional, maybe even diverting from the main point: The perfect separability of the classes.

```{r}
screeplot(pca, type = "l", npcs = 15, main = "Screeplot")
abline(h = 1, col="blue", lty=1)
# What means Eigenvalue =1 ?? Change the legend!!
legend("topright", legend=c("Eigenvalue = 1"), col=c("blue"), lty=1, cex=0.6)
cumpro <- cumsum(pca$sdev^2 / sum(pca$sdev^2)) 
plot(cumpro[0:15], xlab = "principal components", ylab = "explained variance", main = "cumulative variance plot")
abline(h = 0.9, col="blue") 
legend("topleft", legend=c("90% expl. var"), col=c("blue"), lty=1, cex=0.6)
```
Direct interpretation of the plots:

* The scree plot has knees / elbows at the 3rd and 7th PC, indicating it might be reasonable to work with the first 2 or 6 PC.

* The scree plot shows that the first 6 PC have eigenvalues larger than 1, indicating it might be reasonable to work with the first 6 PCs.

* The cumulated variance plot marks the first 6 PC explaining almost 90% of our data set's variance, indicating it might be reasonable to work with the first 6 or 7 PCs.

calculate and list the explained variance (to get the exact numbers not obvious from the plots)

```{r}
v <- sum(pca$sdev^2)
round(100 * pca$sdev^2 / v , 1)
```

```{r}
plot(pca$x[,1],pca$x[,2], xlab="PC1 (44.7%)", ylab = "PC2 (20%)", main = "PC1 / PC2 - plot", pch=3 , col=train$diagnosis)
```
zoom in
```{r}
plot(pca$x[,1],pca$x[,2], xlab="PC1 (44.7%)", ylab = "PC2 (20%)", main = "PC1 / PC2 - plot", pch=3 , col=train$diagnosis , xlim=c(-2.5,2.5) , ylim=c(-2.5, 2.5) )
```

This shows pretty well that the two classes are well separated. This suits the feedback from the logistic regression.

We could build a classification from PCA alone. I guess.

## to maybe do: visualise as Bubblechart
Use the 3rd PC as point size in a ggplot bubblechart.

## Classifier from PCA

```{r}
library(ggplot2)

ggplot(train, aes(x = pca$x[,1], y = pca$x[,2], color = diagnosis)) +
  geom_point(size = 2) +
  theme_minimal() + 
  stat_ellipse(geom="polygon", aes(fill = diagnosis), 
                      alpha = 0.2,
                      show.legend = FALSE, 
                      level = 0.95) + 
  ggtitle("training data points in training ellipses")

```

```{r}
# project new data onto the PCA space
test.pr <- scale(test[-1], pca$center, pca$scale) %*% pca$rotation # test data projected

# we don't need another visualisation, just the scatter plot of training data
#plot(test.pr[,1],test.pr[,2], xlab="PC1", ylab = "PC2", main = "test data", col=test$diagnosis)
```

We have to plot the test data in the training data PCA plot to get an idea of the classification. We should take the ellipses from the training data PCA plot and the test data points and predict according to the ellipse the point falls in.

```{r}
# create a dataframe for train and test in the PCA coordinate system with the split and diagnosis as additional columns and info

ptrain <- data.frame(pca$x)
ptrain$split <- 1
ptrain$diagnosis <- train$diagnosis
ptest <- data.frame(test.pr)
ptest$split <- 0
ptest$diagnosis <- test$diagnosis
dsp <- rbind(ptrain,ptest)
dim(dsp)
```

```{r}
ggplot(dsp, aes(x=PC1,y=PC2, fill = diagnosis), subset=(split=='1')) +
  stat_ellipse(geom = "polygon", alpha=0.6) +
  geom_point(aes(x=PC1, y=PC2, col=diagnosis), data= subset(dsp, split=='0'))+
  ggtitle("test data points in training ellipses")
```
```{r}
ggplot(dsp, aes(x=PC1,y=PC2, fill = diagnosis), subset=(split=='1')) +
  stat_ellipse(geom = "polygon", alpha=0.6) +
  geom_point(aes(x=PC1, y=PC2, col=diagnosis), data= subset(dsp, split=='0'))+
  ggtitle("test data points in training ellipses, zoom") + 
  coord_cartesian(xlim = c(-2.5,2.5), ylim=c(-2.5,2.5)) 
```
I would make a prediction on KNN in the full PCA-coordinate system. 
This is not what was asked for. But a visualisation was asked for!

# LDA
Does it make sense to apply LDA? James at al p 138: Logistic regression is unstable for well separated classes, LDA is not. LDA is more stable than logistic regression in the case of

* small dataset (n small)

* "Distribution of the predictors is approximately normal in each of the classes" (James p. 138)

## SVM?