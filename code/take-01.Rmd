---
title: 'ML2 project: breast cancer'
output: html_notebook
author: ['Silke Meiner','Rafaela Neff'] # alphabetical order
---
## Dataset

[Breast Cancer Wisconsin (Diagnostic) Data Set](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data)

### Binary Classification Task

_Predict whether the cancer is benign or malignant._ 

Classes: **M = malignant** and  **B = benign**.

For computational reasons those values are be encoded as Integers: **1 = M = malignant** or **0 = B = benign**.
 

```{r}
#change working directory to current dir
wd = getwd()
setwd(wd)
#read data
ds <- read.csv('data/data.csv')
dim(ds)
str(ds)
# summary(ds)
```

```{r fig.height=3, fig.width=3}
barplot(table(ds$diagnosis), main='balance of target variable')
```

exact numbers
```{r}
# ds %>% count(diagnosis)
table(ds$diagnosis)
sum(table(ds$diagnosis))
```
No missing values in the diagnosis, the target value.

Our data is not exactly fully balanced but it could be much worse. We are not in an unbalanced situation as in the usual fraud detection setting.

# Checking for missing data
```{r}
library(mice)
md.pattern(ds, rotate.names = TRUE)
```
For the variable X all data is missing. We will remove this variable. 
There are no further missing values. 

# Checking for duplicates
```{r}
duplicated(ds)
```
There are no duplicates in the data set

# Dataset description
We have a data set of 33 variables, of which

* binary variable diagnosis will be the target variable

* 30 numerical predictor variables / features 

* the ID is not a predictor / feature

* X will be removed because it is empty
 
 
# Correlation
```{r fig.height=8, fig.width=8}
# install.packages("ggcorrplot")
library(ggcorrplot)
corr <- round(cor(ds[,3:32]), 3)
# ggcorrplot(corr)
ggcorrplot(corr, hc.order = TRUE, outline.col = "white") # uses hierarchical clustering to order variables
```
Some variables seem highly (pos.) correlated and the number of variables can maybe be reduced.

# Scaling
Our variables are at very different scalings (orders of magnitude).
```{r}
boxplot(ds[,3:33])
```
Look into variables with the largest medians:
```{r}
# get a closer look at the variables with a large median

# apply(ds[,3:32], 2, mean)
vars1 <- which(apply(ds[,3:32], 2, median) > 300)
#vars1
boxplot(ds[,2+vars1],'highest median variables') # 2+ because we initially removed the first 2 variables
```
```{r}
# of those the ones with a relatively large median, around 100
# boxplot(ds[,c(5,16,25)])
```
```{r}
# max values (outliers) below 50
# boxplot(ds[,c(3:4,7:15,17:24,27:33)])
```
and look into the variables with the smallest maximal values
```{r}
# and the small medians only
# apply(ds[,3:32], 2, median)
vars1 <- which(apply(ds[,3:32], 2, max) < 0.1)
#vars1
boxplot(ds[,2+vars1],main='variables with the smallest maximal values') # 2+ because we initially removed the first 2 variables
```
 We need to scale our data. This could be done using min-/max-scaling.
 
# Proposed Methods
 
## Logistic regression
 
 The first methode proposed  is _Logistic Regression_. The hypothesis model calculates the probability of the Diagnosis outcome being 0 or 1.
 
### Evaluation

## Neural Network

 As second method we'd propose a _Neural Network_ for binary classification, for example implemented using the library nnet or Keras.

 
### Evaluation 

We will evaluate both approaches as follows:
 
 - Confusion Matrix
 
 - F1 score
 
 - Accuracy
