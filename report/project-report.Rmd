---
title: "Machine Learning 2 Project: Breast cancer prediction from tabular data"
author: "Silke Meiner, Rafaela Neff"
date: "Winter Semester 2020/21"
output:
  pdf_document:
    toc: true
    citation_package: natbib
  bookdown::pdf_book:
    citation_package: biblatex

bibliography: references.bib
csl: vancouver.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Abstract

We present and compare machine-learned classification algorithms for diagnosing breast tumor cells as benign or malignant. We have trained on tabular data consisting of features extracted from microscopic images of fine-needle aspirates/biopsies.

We got reasonably good results from Logistic Regression and were able to improve these results through single and multi-layer neural networks. The final neural network with an accuracy of 0.982 (\98%), a sensitivity of 0.976, and a specificity of 0.986 is being audited at the German Gesundheitsministerium to become a state-approved diagnostic tool.

The layout of this paper follows the CRISP-DM model. 

# Business Understanding

Breast cancer is the most common cancer for women and ranks highest for cancer-related deaths in women in Germany: In 2016, there were 68,950 women and 710 men who have Breast Cancer (ICD-10 C50). In 2020 18,570 women and 1,1 men have died of breast cancer. The situation is similar in many other countries.

Tumors can build in the human body as some cells grow more than they usually should. If this growth is not limiting itself and destroys body tissue, and hinders body functions, the tumor is labeled malignant and called cancer.

Tumors are classified in a binary fashion as either malignant or benign. Their difference in microscopic imagery is shown in figure 1. A typical first step when diagnosing a tumor is to do a fine needle aspirate (FNA) of a breast mass and looking at the cells through a microscope, describing the characteristics of the cell nuclei. Further treatment differs according to the tumor diagnosis as benign or malignant. 

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{images/normal-vs-cancerous-cells.png}
    \caption{benign / normal and malignant / cancerous  cells}
    \label{fig:normal-vs-cancer}
\end{figure}

In medical diagnostics, based on human or artificial intelligence, correctness is very important. It is described in terms of sensitivity and specificity. Both are measured in percentage. Sensitivity is the true positive rate, the rate at which malignant tumors are correctly identified. Specificity is the true negative rate, the rate at which a patientâ€™s concerns can correctly be relieved.

Sources : 

* \cite{rki}

* \cite{malignant-and-benign}

* \cite{cancer-cells-vs-normal}


# Data Sources and Data Understanding

The data for this project was collected in 1995 by the University of Wisconsin and made available to us through Prof. Dr. Nick Street of the University of Iowa. 

The data can be downloaded from the University of California, Irvine, https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29

## Data Understanding

Our data set is mid-sized with 569 observations for 30 numeric feature variables. In addition, each observation has a binary diagnosis as benign or malignant. The data set is slightly unbalanced, with 357 (63\%) benign and 212 (37\%) malignant cases. We made malignant the positive class. 

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{images/binary-classification.png}
    \caption{classes in the binary classification task, B: benign, M: malignant / cancerous}
    \label{fig:bin_class}
\end{figure}

In figure \ref{fig:cor-1} we present the correlation of features and find some features / predictor variables highly correlated.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{images/correlation-features.png}
    \caption{correlation of feature variables}
    \label{fig:cor-1}
\end{figure}

We point out three groups of correlated features:

The group of most highly (and positively) correlated features correspond to the description of malignant / cancerous cells given in \ref{fig:normal-vs-cancer}: They describe cells wit a large and irregular perimeter, radius and area:

* perimeter_mean, perimeter_worst
* radius_mean, radius_worst
* area_mean, area_worst

The second most highly (and positively) correlated group of features are the standard errors for the cells perimeter, radius and area, underlying the group of most highly correlated features.

* perimeter_se
* radius_se
* area_se

The third group of high to mild positive correlation is for features related to concavity and compactness. 

* concave.points_mean, concave.points_worst
* concavity_mean, concavity_worst
* compactness_mean, compactness_worst

In figure \ref{fig:normal-vs-cancer} convexity is a feature in the benign / normal cells. Its opposite, concavity, can easily be associated with the malignant / cancerous cells.

When there is a (positive) correlation of feature variables with the target variable, it is most prominent for features from the first and third group of correlated features. A positive correlation of target and features is maximal for concave.points_worst with a value of 0.794.

In figure  \ref{fig:densities} we present two density plots of our features. Both features have a high (positive) correlation to the target. Distributions are well distinguishable for the two classes, which is an advantage for the classification task.

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{images/density2.png}
    \includegraphics[width=0.45\textwidth]{images/density3.png}
    \caption{Density plots for two exemplary features}
    \label{fig:densities}
\end{figure}

For more density plots, please see our data-understanding.Rmd notebook.

The features in this data set seem handcrafted and contain expert domain knowledge.

## Data Visualization looped back after first modeling

This visualization of our data by applying PCA was developed after a first loop in the CRISP model. In sequential reading, this serves as a general visualization now and will also be relevant later on. 

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{images/A-training-full-PCA.png}
    \caption{PCA for the full training data}
    \label{fig:A-training-full-PCA}
\end{figure}

# Data Preparation

Since there were no missing data in our set, we did not impute anything. Data was used as delivered.

Data was separated into training and test sets, each with a separate file. The data was split 80/20 and with stratification wrt to the diagnosis.

For further details, please look into our notebook data-preparation.Rmd .

# Modeling
To solve the classification task, we applied two machine learning methods: Logistic Regression and neural networks.

In classification tasks, we generally have predictor variables and a target variable. In binary classification, the target variable can take one of two distinct values, interpreted as the two classes on option.

The methodological similarities of both machine learning models are in the training and evaluation of the model. The training requires a training set of observed data points, including the true values of the target variable. For evaluation, a test set of observed  data points including the true values of the target variable is required. Training and test set need to be disjoint. On the test set, the algorithm predicts classes for the data points, and predictions are compared with the targets. We are counting correct and not correct classifications and setting them in relation results in accuracy, sensitivity, and specificity as measures of success.

## Logistic Regression

### Mathematical Background

**Given**: Some numeric data, in tabular form of \(n\) rows and \(p+1\) variables. \(p\) variables are predictors, and the remaining variable is the target. The target takes one of two values, interpreted as two classes, with one class defined as the positive class.

**Desired**: The class a particular data point belongs to with a probability distribution over the two classes (stating the probabilities that the data point belongs to each class). Alternatively, the following result would be of equal value: if or if not, the data point belongs to the positive class and the associated probability (single value in [0,1]).

Linear regression uses a linear combination of the variables to predict another numeric target variable. Logistic Regression does linear regression for the log-odds of the desired probabilities.

$$
\text{log-odds} = \beta_0 + \beta_1 x_1 + \dots + \beta_p x_p     
$$

The log-odds are transformed into (conditional) probabilities for the positive class through the logistic function $$\sigma(.)$$, also called sigmoid.

$$
p(\bf{x}) = \sigma(\text{log-odds}) = \sigma(\beta_0 + \beta_1 x_1 + \dots + \beta_p x_p ) 
$$

with \( \bf{x}= ( x_1, \dots, x_p) \) , \( p(\bf{x}) = \mathbf{P}\left(\text{target = positive class} | \text{predictors } = \bf{x} \right) \) and  \( \sigma(a) = \frac{1}{1+e^{-a}} \).

If the probability for the positive class \( p(\bf{x}) \)  exceeds a threshold, the data point is classified as the positive class.

The performance of the model is determined by its coefficients/weights \( \beta \). Finding the best / suitable coefficients is done in training. For Logistic Regression, there are several training methods performed by statistical software like R.

At the beginning of running a Logistic Regression, we ran into a 'problem' hinting at a perfect separation of classes in our data set. We decided to ignore this and try to get the best possible results from Logistic Regression. Another option would have been changing to support vector machines that could directly exploit our data's separability.

Sources:

Log Regression (\cite{logreg})

for ignoring the problem of complete separation
(\cite{ucla})

Finding coefficients for Logistic Regression:
(\cite{newton})

### Modeling in R

First attempts to run a Logistic Regression are documented in log-reg-01.Rmd and log-reg-02.Rmd in the GitHub repository's code folder. The following presentation is based on log-reg-03.Rmd and uses the caret library in R.

We compared different preprocessing options for the Logistic Regression:

* no preprocessing
* center and scaling
* min-max-normalization to the interval [0,1]
* PCA (including center and scaling)

Using 10-fold cross-validation, which was repeated 50 times (with new and different separation of the training data into ten subsets/folds, one of which would be used for validation), we obtained the following plot \ref{fig:pre-proc-options} for accuracies for the different preprocessing options.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{images/preprocessing-options.png}
    \caption{accuracy for different pre-processing options}
    \label{fig:pre-proc-options}
\end{figure}

Principal component analysis (PCA) shows by far the best result.

We also tested if removing correlated variables improves accuracy. That can happen because removing highly correlated variables can eliminate noise, making it easier for the model to pick up relevant structures in the data.

When looking at a group of highly correlated features, all but one feature of the group was removed. 

We tested removing features with 99% correlation and with 97% correlation against the complete feature set. We tried this for the Logistic Regression with PCA preprocessing and the Logistic Regression with no preprocessing (cf figure \ref{fig:feature-reduction-options}), arriving at the following results:

* Removing highly correlated variables seems to improve the Logistic Regression algorithm's accuracy on data that has not been preprocessed.
* Removing highly correlated variables may or may not have an effect or a positive effect on accuracy for the Logistic Regression algorithm on data that has been preprocessed with PCA.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{images/feature-selection.png}
    \caption{accuracy for different reduced feature number}
    \label{fig:feature-reduction-options}
\end{figure}

That makes sense, as the PCA is a transformation of the data that creates new variables that explain the data's variance in descending order. High correlation with previous principal components will not help with explaining the remaining variance.

As the overall best model, we keep the best model from Logistic Regression with a PCA as preprocessing and the full set of variables (mainly, we don't have a good enough reason to reduce features and throw away information, especially when the PCA reduces information in a reasonable, statistically approved way).

We arrived at our best model: logistic regression with PCA preprocessing and the whole feature set. We then trained the model again on the full training data. summary of the model is shown in figure \ref{fig:logR-final-model}. The model did converge after ten iterations; some of the principal components are highly significant below 0.1\%.

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{images/logR_final_model.png}
    \caption{Final model for Logistic Regression}
    \label{fig:logR-final-model}
\end{figure}

During cross-validation, we obtained a mean accuracy of 96.5% during training and a mean sensitivity and specificity of 98.2% (same for both). In general, these are very acceptable results. In the medical domain, we should aim for better results, most important, a better sensitivity.

### Explainability

Logistic regression is a well explainable model. The \( \beta \) coefficients show the change in the log-odds for each feature variable under the condition of all else equal. In our case, with PCA preprocessing, we get coefficients that show the change in the log-odds for each principal component. Since principal components are linear combinations of the original features, the all else equal condition will not generally hold/be less realistic.

We state the results of the variable importance function (varImp) in R caret in figure \ref{fig:logR_var_imp}

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{images/logR_varImportance.png}
    \caption{variable importance for Logistic Regression with PCA}
    \label{fig:logR_var_imp}
\end{figure}

We try to apply black box interpretation models like variable shuffling.
## Artificial Neural Networks

### Mathematical Background

Artificial Neural Networks are forecasting methods based on simple mathematical models of the brain. They allow complex nonlinear relationships between the response variable and its predictors. (\cite{otexts}) 

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{images/ann.jpg}
    \caption{Sample artificial neural network architecture (not all weights are shown) - (\cite{ann})}
    \label{fig:ann}
\end{figure}

Connections between neurons are weighted to represent the connection strength. It's the artificial pendant of the synapses in the brain. Positive weights are used to excite neurons in the network, and negative weights are used to inhibit other neurons.

**Architecture**: Topology of the network

**Activities**: How do ANNs Neurons respond to one another to produce a particular behavior.

**Learning Rule**: How should weights and connections change regarding the input, output, and error-rate.

**Deep Neural Network**: Network with many hidden layers.

ANN structure can be described as its Architecture, Activities, and Learning Rule.

In the model the weighted combination of input signals \(Î± =\sum_{i=1}^n w_ix_i+b\) is aggregated and the output \(f(Î±)\) is transmitted by the neurons. The function \(f\) is non-linear. The neuron's activation threshohold is represented by the bias \(b\) as shown in figure  \ref{fig:weights-and-bias}.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{images/weights-and-bias.png}
    \caption{One single neuron with weights, bias and its activation}
    \label{fig:ann}
\end{figure}

We will be focusing on the feedforward architecture used by H2o.

\begin{center}
Activation Functions in H2o
 \begin{tabular}{| c |  c | c|} 
 \hline
 Function & Formula & Range \\ [0.5ex] 
 \hline
 \hline
Tanh &  \(f(Î±) =\frac{e^Î±âˆ’e^{âˆ’^Î±}} {eÎ±+eâˆ’Î±}\) & \(f(Â·) âˆˆ [âˆ’1, 1]\) \\ 
Rectified Linear & \(f(Î±) = max(0, Î±)\) & \(f(Â·) âˆˆ R_+\) \\ 
Maxout & \(f(Î±_1, Î±_2) = max(Î±_1, Î±_2)\) &  \(f(Â·) âˆˆ R\) \\
 \hline
\end{tabular}
\end{center}

For each training example, a loss function is minimized, so the labeled training data's error gets smaller. For our classification task, we are using H2o's Cross-Entropy Loss denoted with the following formula:
$$
L(W, B|j) = âˆ’ \sum_{yâˆˆO} ln(o^{(j)}_y)) Â· t^{(j)}_y + ln(1 - o^{(j)}_y)Â· (1 - t^{(j)}_y)
$$
(\cite{h2oDL})

### Modeling in R

First attempts to run a fit a Neural Network in R are documented in nnet.R and nnetCV.R, the GitHub repository's code folder. The following presentation is based on h2o.R and uses the R-Package h2o.

We compared different preprocessing options for the Neural Network:

* no preprocessing for nnet
* min-max-normalization to the interval [0,1]  for nnet
* Standardization for h2o

### Proposed Model architecture

We arrived at the selected Hyperparameters by performing a grid-search:

Activation function: Rectified Linear
Input drop out ratio: 0.2
Hidden layers : 3 sets each consisting of 3 layers
Epoch: 500
Nfolds: 10

We've also tried Tanh and Maxout as the activation function in the last iteration of our project. Both activation functions didn't improve the model's performance significantly but are shown in the source code in file _h2o.R_.

The classes were balanced during training using the balance_class parameter of the h2o-package. The balancing is done by oversampling the minority class. We've validated our models using 10-fold cross-validation with a stratified assignment to keep the classes' proportion as they were in the original dataset. 

We've chosen the model with the highest AUC-value, which isn't 1 for the training loop. This decision is based on preventing overfitting. The False-Negatives were examined manually to get a grip on the real-world-value of our model.  We are trying to avoid that our model tells us that a person doesn't have breast cancer when he or she does.  

Considering all this we did arrive at the following architecture:

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{images/model_details.png}
    \caption{Summary of the choosen Artificial Neural Network after training}
    \label{fig:model_details}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{images/variable_importance.png}
    \caption{Variable Importance of the choosen Artificial Neural Network}
    \label{fig:variable_importance}
\end{figure}


### Explainability

We did choose the R-Package h2o over nnet for its great explainability.

#### Expert Plots

A deeper understanding of a fitted model can be gained with a variety of plots. For an ANN, a Partial Dependence Plot or the Individual Conditional Expectation Plot of a given column can be visualized.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{images/pd_plot.png}
    \caption{Partial Dependence Plot wrt area\_worts}
    \label{fig:pd_plot}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{images/ice_plot.png}
    \caption{Conditional Expectation Plot wrt area\_worst}
    \label{fig:ice_plot}
\end{figure}

# Final Assessment

We compare the logistic regression and the neural networks wrt accuracy and sensitivity and specificity on the test set.

\begin{center}
 \begin{tabular}{|c | c |  c | c|} 
 \hline
 Model & Accuracy & Sensitivity/Recall & Specificity \\ [0.5ex] 
 \hline
 \hline
 Logistic regression & .965 & .929 & .986 \\ 
 \hline
 Neural Network & .982 & .976 &  .986 \\
 \hline
\end{tabular}
\end{center}

The neural network performs better regarding accuracy with only two misclassifications on the test set, whereas the logistic regression misclassifies in four cases. The error difference is due to the neural network's better sensitivity: only one malignant tumor was wrongly classified, whereas the logistic regression did err in three cases.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{images/h2o_model_performance.png}
    \caption{Metrices of the choosen Artificial Neural Network}
    \label{fig:h2o_model_performance}
\end{figure}

Please, see the notebooks (of the specific model) for more details.

# Future Work

Interpretation and explainability are essential topics in machine learning and a fundamental requirement in the medical domain. On the one hand, we would like to improve the accuracy and especially the neural network model's sensitivity. On the other hand, we would like to investigate (and improve) the connection between the logistic regression and the neural network and use the logistic regression to obtain interpretation and explanations. If we can find and point out connections of logistic regression and neural network interpretation and explanation from logistic regression can be meaningful for the neural network, too. 

Shifting the focus in logistic regression from performance to interpretability, we would drop the PCA preprocessing. Instead of feature reduction by PCA, we would start with the neural network output features as important variables.

Why we think there is a connection between logistic regression and neural networks: A neural network with sigmoid activation is a collection of logistic regression models: Every node in the first hidden layer is a Logistic Regression. We will obtain a classification algorithm if we connect the hidden layer node with the suitable loss function. The neural network can be more powerful since it uses several nodes in its (first) hidden layer. Additional power requires these individual regressions to act differently and be combined in a meaningful way (if all logistic regressions did the same, there would be no improvement). This task has to be solved by the network through optimization concerning the final classification. We want to investigate this further. 

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{images/logistic_regression_and_neural_network.jpeg}
    \caption{Metrices of the choosen Artificial Neural Network}
    \label{fig:h2o_model_performance}
\end{figure}
(\cite{nn-better-logReg})

Interpretation and explanations for logistic regression are already available. We would start applying them to our task.

# References


